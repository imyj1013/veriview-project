#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VeriView AI ì„œë²„ - ë©”ì¸ ì„œë²„ (ëª¨ë“  AI ëª¨ë“ˆ í¬í•¨)
LLM, OpenFace2.0, Whisper, TTS, Librosa ê¸°ëŠ¥ì„ ëª¨ë‘ í™œìš©í•˜ëŠ” ì‹¤ì œ ì„œë²„

ì‹¤í–‰ ë°©ë²•: python main_server.py
í¬íŠ¸: 5000
"""
from flask import Flask, jsonify, request, send_file
from flask_cors import CORS
import os
import tempfile
import logging
import time
import json
from typing import Dict, Any, Optional

# ì‹¤ì œ AI ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from interview_features.debate.llm_module import DebateLLMModule
    from interview_features.debate.openface_integration import OpenFaceDebateIntegration
    LLM_MODULE_AVAILABLE = True
    OPENFACE_INTEGRATION_AVAILABLE = True
    print("âœ… ì‹¤ì œ AI ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ: LLM ë° OpenFace í†µí•© ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âš ï¸ ì‹¤ì œ AI ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    LLM_MODULE_AVAILABLE = False
    OPENFACE_INTEGRATION_AVAILABLE = False

# ë°±ì—… ëª¨ë“ˆ ì„í¬íŠ¸ (ì‹¤ì œ ëª¨ë“ˆì´ ì—†ì„ ê²½ìš°)
try:
    from test_features.debate.main import DebateTestMain
    from test_features.personal_interview.main import PersonalInterviewTestMain
    BACKUP_MODULES_AVAILABLE = True
    print("âœ… ë°±ì—… ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ: í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âŒ ë°±ì—… ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    BACKUP_MODULES_AVAILABLE = False

# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from modules.job_recommendation_module import JobRecommendationModule
    job_recommendation_module = JobRecommendationModule()
    JOB_RECOMMENDATION_AVAILABLE = True
    print("âœ… ê³µê³ ì¶”ì²œ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ê³µê³ ì¶”ì²œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    JOB_RECOMMENDATION_AVAILABLE = False
    job_recommendation_module = None

# TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from modules.tfidf_job_recommendation_module import TFIDFJobRecommendationModule
    tfidf_recommendation_module = TFIDFJobRecommendationModule()
    TFIDF_RECOMMENDATION_AVAILABLE = True
    print("âœ… TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TFIDF_RECOMMENDATION_AVAILABLE = False
    tfidf_recommendation_module = None

# ê°œë³„ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    import whisper
    import librosa
    import soundfile as sf
    WHISPER_AVAILABLE = True
    LIBROSA_AVAILABLE = True
    print("âœ… Whisper ë° Librosa ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ìŒì„± ë¶„ì„ ëª¨ë“ˆ ì¼ë¶€ ì œí•œ: {e}")
    WHISPER_AVAILABLE = False
    LIBROSA_AVAILABLE = False

# TTS ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from TTS.api import TTS
    TTS_AVAILABLE = True
    print("âœ… TTS ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ TTS ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TTS_AVAILABLE = False

# Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
llm_module = None
openface_integration = None
debate_test_system = None
personal_interview_test_system = None
whisper_model = None
tts_model = None

def initialize_ai_systems():
    """AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global llm_module, openface_integration, debate_test_system, personal_interview_test_system
    global whisper_model, tts_model
    
    try:
        # LLM ëª¨ë“ˆ ì´ˆê¸°í™”
        if LLM_MODULE_AVAILABLE:
            llm_module = DebateLLMModule(llm_provider="openai", model="gpt-4")
            logger.info("LLM ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # OpenFace í†µí•© ëª¨ë“ˆ ì´ˆê¸°í™”
        if OPENFACE_INTEGRATION_AVAILABLE:
            openface_integration = OpenFaceDebateIntegration()
            logger.info("OpenFace í†µí•© ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°±ì—… ëª¨ë“ˆ ì´ˆê¸°í™” (ì‹¤ì œ ëª¨ë“ˆì´ ì—†ì„ ê²½ìš°)
        if BACKUP_MODULES_AVAILABLE:
            if not LLM_MODULE_AVAILABLE:
                debate_test_system = DebateTestMain()
                logger.info("í† ë¡ ë©´ì ‘ ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            personal_interview_test_system = PersonalInterviewTestMain()
            logger.info("ê°œì¸ë©´ì ‘ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Whisper ëª¨ë¸ ì´ˆê¸°í™”
        if WHISPER_AVAILABLE:
            whisper_model = whisper.load_model("base")
            logger.info("Whisper ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # TTS ëª¨ë¸ ì´ˆê¸°í™”
        if TTS_AVAILABLE:
            tts_model = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")
            logger.info("TTS ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
    except Exception as e:
        logger.error(f"AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

def process_audio_with_librosa(audio_path: str) -> Dict[str, Any]:
    """Librosaë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ë¶„ì„"""
    if not LIBROSA_AVAILABLE:
        return {"error": "Librosa ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    try:
        y, sr = librosa.load(audio_path)
        
        # ê¸°ë³¸ ìŒì„± íŠ¹ì„± ì¶”ì¶œ
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
        
        # í‰ê· ê°’ ê³„ì‚°
        analysis_result = {
            "mfcc_mean": float(mfccs.mean()),
            "spectral_centroid_mean": float(spectral_centroids.mean()),
            "spectral_rolloff_mean": float(spectral_rolloff.mean()),
            "zero_crossing_rate_mean": float(zero_crossing_rate.mean()),
            "duration": float(len(y) / sr),
            "sample_rate": int(sr),
            "voice_stability": min(1.0, max(0.0, 1.0 - abs(spectral_centroids.std() / spectral_centroids.mean()))),
            "speaking_rate_wpm": estimate_speaking_rate(y, sr),
            "volume_consistency": calculate_volume_consistency(y),
            "fluency_score": calculate_fluency_score(y, sr)
        }
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"Librosa ì˜¤ë””ì˜¤ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return {"error": f"ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

def estimate_speaking_rate(audio_data, sample_rate):
    """ë§í•˜ê¸° ì†ë„ ì¶”ì • (ë‹¨ìœ„: WPM)"""
    try:
        # ê°„ë‹¨í•œ ìŒì„± í™œë™ ê°ì§€
        frame_length = int(0.025 * sample_rate)  # 25ms
        hop_length = int(0.01 * sample_rate)     # 10ms
        
        # ì—ë„ˆì§€ ê¸°ë°˜ ìŒì„± í™œë™ ê°ì§€
        energy = librosa.feature.rms(y=audio_data, frame_length=frame_length, hop_length=hop_length)[0]
        energy_threshold = energy.mean() * 0.3
        
        # ìŒì„± êµ¬ê°„ ê³„ì‚°
        speech_frames = len(energy[energy > energy_threshold])
        speech_duration = speech_frames * hop_length / sample_rate
        
        # ëŒ€ëµì ì¸ WPM ê³„ì‚° (í‰ê·  í•œêµ­ì–´ ìŒì ˆ/ë‹¨ì–´ ë¹„ìœ¨ ê³ ë ¤)
        estimated_wpm = max(60, min(200, (speech_duration / 60) * 150))
        
        return estimated_wpm
        
    except Exception:
        return 120  # ê¸°ë³¸ê°’

def calculate_volume_consistency(audio_data):
    """ìŒëŸ‰ ì¼ê´€ì„± ê³„ì‚°"""
    try:
        # RMS ì—ë„ˆì§€ ê³„ì‚°
        frame_length = 2048
        hop_length = 512
        rms_energy = librosa.feature.rms(y=audio_data, frame_length=frame_length, hop_length=hop_length)[0]
        
        # í‘œì¤€í¸ì°¨ ê¸°ë°˜ ì¼ê´€ì„± ì ìˆ˜
        consistency = 1.0 - min(1.0, rms_energy.std() / (rms_energy.mean() + 1e-8))
        return max(0.0, consistency)
        
    except Exception:
        return 0.5  # ê¸°ë³¸ê°’

def calculate_fluency_score(audio_data, sample_rate):
    """ë°œí™” ìœ ì°½ì„± ì ìˆ˜ ê³„ì‚°"""
    try:
        # ë¬´ìŒ êµ¬ê°„ ê°ì§€
        frame_length = int(0.025 * sample_rate)
        hop_length = int(0.01 * sample_rate)
        
        energy = librosa.feature.rms(y=audio_data, frame_length=frame_length, hop_length=hop_length)[0]
        silence_threshold = energy.mean() * 0.1
        
        # ë¬´ìŒ êµ¬ê°„ ë¹„ìœ¨ ê³„ì‚°
        silence_ratio = len(energy[energy < silence_threshold]) / len(energy)
        
        # ìœ ì°½ì„± ì ìˆ˜ (ë¬´ìŒì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        fluency_score = 1.0 - min(1.0, silence_ratio * 2)
        return max(0.0, fluency_score)
        
    except Exception:
        return 0.5  # ê¸°ë³¸ê°’

def transcribe_with_whisper(audio_path: str) -> Dict[str, Any]:
    """Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹"""
    if not WHISPER_AVAILABLE or whisper_model is None:
        return {"error": "Whisper ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    try:
        result = whisper_model.transcribe(audio_path, language="ko")
        
        return {
            "text": result["text"],
            "language": result["language"],
            "segments": result["segments"][:3],  # ì²˜ìŒ 3ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ
            "confidence": calculate_transcription_confidence(result)
        }
        
    except Exception as e:
        logger.error(f"Whisper ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {str(e)}")
        return {"error": f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {str(e)}"}

def calculate_transcription_confidence(whisper_result):
    """Whisper ê²°ê³¼ì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
    try:
        if "segments" in whisper_result and whisper_result["segments"]:
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            total_confidence = 0
            total_segments = 0
            
            for segment in whisper_result["segments"]:
                if "avg_logprob" in segment:
                    # logprobë¥¼ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
                    confidence = max(0, min(1, (segment["avg_logprob"] + 1) / 1))
                    total_confidence += confidence
                    total_segments += 1
            
            if total_segments > 0:
                return total_confidence / total_segments
        
        return 0.75  # ê¸°ë³¸ ì‹ ë¢°ë„
        
    except Exception:
        return 0.75

def synthesize_with_tts(text: str, output_path: str) -> Optional[str]:
    """TTSë¥¼ ì‚¬ìš©í•œ ìŒì„± í•©ì„±"""
    if not TTS_AVAILABLE or tts_model is None:
        logger.warning("TTS ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(text) > 500:
            text = text[:500] + "..."
        
        tts_model.tts_to_file(text=text, file_path=output_path)
        
        if os.path.exists(output_path):
            return output_path
        else:
            return None
            
    except Exception as e:
        logger.error(f"TTS ìŒì„± í•©ì„± ì˜¤ë¥˜: {str(e)}")
        return None

def extract_audio_from_video(video_path: str) -> Optional[str]:
    """ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ"""
    try:
        import subprocess
        
        # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        audio_path = video_path.replace('.mp4', '_audio.wav').replace('.webm', '_audio.wav')
        
        # FFmpegë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        command = [
            'ffmpeg', '-i', video_path, '-acodec', 'pcm_s16le', 
            '-ac', '1', '-ar', '16000', audio_path, '-y'
        ]
        
        subprocess.run(command, check=True, capture_output=True)
        
        if os.path.exists(audio_path):
            return audio_path
        else:
            return None
            
    except Exception as e:
        logger.error(f"ì˜¤ë””ì˜¤ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return None

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.route('/ai/test', methods=['GET'])
def test_connection():
    """ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    logger.info("ì—°ê²° í…ŒìŠ¤íŠ¸ ìš”ì²­ ë°›ìŒ: /ai/test")
    
    # ë°±ì—”ë“œ ì—°ê²° í…ŒìŠ¤íŠ¸
    backend_status = "ì—°ê²° í™•ì¸ í•„ìš”"
    try:
        import requests
        response = requests.get("http://localhost:4000/api/test", timeout=2)
        if response.status_code == 200:
            backend_status = "ì—°ê²°ë¨ (ì •ìƒ ì‘ë™)"
        else:
            backend_status = f"ì—°ê²°ë¨ (ìƒíƒœ ì½”ë“œ: {response.status_code})"
    except Exception as e:
        backend_status = f"ì—°ê²° ì‹¤íŒ¨: {str(e)}"
    
    test_response = {
        "status": "AI ë©”ì¸ ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
        "server_type": "main_server (ëª¨ë“  AI ëª¨ë“ˆ í¬í•¨)",
        "backend_connection": backend_status,
        "modules": {
            "llm": "ì‚¬ìš© ê°€ëŠ¥" if LLM_MODULE_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€",
            "openface": "ì‚¬ìš© ê°€ëŠ¥" if OPENFACE_INTEGRATION_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€",
            "whisper": "ì‚¬ìš© ê°€ëŠ¥" if WHISPER_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€",
            "tts": "ì‚¬ìš© ê°€ëŠ¥" if TTS_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€",
            "librosa": "ì‚¬ìš© ê°€ëŠ¥" if LIBROSA_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€",
            "job_recommendation": "ì‚¬ìš© ê°€ëŠ¥" if JOB_RECOMMENDATION_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€",
            "tfidf_recommendation": "ì‚¬ìš© ê°€ëŠ¥" if TFIDF_RECOMMENDATION_AVAILABLE else "ì‚¬ìš© ë¶ˆê°€"
        },
        "endpoints": {
            "debate": {
                "start": "/ai/debate/start",
                "ai_opening": "/ai/debate/<debate_id>/ai-opening",
                "opening_video": "/ai/debate/<debate_id>/opening-video"
            },
            "personal_interview": {
                "start": "/ai/interview/start",  
                "question": "/ai/interview/<interview_id>/question",
                "answer_video": "/ai/interview/<interview_id>/answer-video"
            },
            "job_recommendation": {
                "recommend": "/ai/jobs/recommend",
                "categories": "/ai/jobs/categories",
                "recruitment_posting": "/ai/recruitment/posting",
                "tfidf_recommend": "/ai/jobs/recommend-tfidf",
                "rare_skills": "/ai/jobs/rare-skills",
                "tfidf_posting": "/ai/recruitment/posting-tfidf"
            }
        },
        "mode": "ì‹¤ì œ AI ëª¨ë“ˆ ì‚¬ìš© (LLM + OpenFace + Whisper + TTS + Librosa)"
    }
    
    logger.info(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response}")
    return jsonify(test_response)

# ==================== ê³µê³ ì¶”ì²œ API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.route('/ai/recruitment/posting', methods=['POST'])
def recruitment_posting():
    """ë°±ì—”ë“œ ì—°ë™ìš© ê³µê³ ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸ (RecruitmentRequest í˜•ì‹ ì§€ì›)"""
    if not JOB_RECOMMENDATION_AVAILABLE and not TFIDF_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        data = request.json or {}
        logger.info(f"ë°±ì—”ë“œ ê³µê³ ì¶”ì²œ ìš”ì²­ ë°›ìŒ: {data}")
        
        # RecruitmentRequest í˜•ì‹ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        user_id = data.get('user_id')
        category = data.get('category', 'ICT')
        
        # TF-IDF ì¶”ì²œì„ ìš°ì„  ì‹œë„
        if TFIDF_RECOMMENDATION_AVAILABLE:
            try:
                # TF-IDFìš© í”„ë¡œí•„ ìƒì„±
                tech_stacks = []
                if data.get('tech_stack'):
                    tech_stacks = [s.strip() for s in data.get('tech_stack').split(',') if s.strip()]
                
                certificates = []
                if data.get('qualification'):
                    certificates = [s.strip() for s in data.get('qualification').split(',') if s.strip()]
                
                majors = []
                if data.get('major'):
                    majors = [s.strip() for s in data.get('major').split(',') if s.strip()]
                if data.get('double_major'):
                    majors.extend([s.strip() for s in data.get('double_major').split(',') if s.strip()])
                
                profile = {
                    'userId': user_id,
                    'techStacks': tech_stacks,
                    'certificateList': certificates,
                    'majorList': majors,
                    'careerYear': data.get('workexperience', 0),
                    'educationLevel': data.get('education', '')
                }
                
                # TF-IDF ëª¨ë“ˆë¡œ ì¶”ì²œ ìƒì„±
                tfidf_result = tfidf_recommendation_module.get_recommendations_by_profile(profile, 10)
                
                if tfidf_result.get('status') == 'success':
                    # TF-IDF ê²°ê³¼ë¥¼ ë°±ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    posting_list = []
                    tfidf_postings = tfidf_result.get('posting', [])
                    
                    for posting in tfidf_postings:
                        posting_list.append({
                            "job_posting_id": posting.get('jobPostingId'),
                            "title": posting.get('title', ''),
                            "keyword": ', '.join(posting.get('techStacks', [])),
                            "corporation": posting.get('corporation', '')
                        })
                    
                    response = {"posting": posting_list}
                    logger.info(f"ë°±ì—”ë“œ TF-IDF ê³µê³ ì¶”ì²œ ì‘ë‹µ: {len(posting_list)}ê°œ ê³µê³ ")
                    return jsonify(response)
                    
            except Exception as e:
                logger.warning(f"TF-IDF ì¶”ì²œ ì‹¤íŒ¨, ê¸°ë³¸ ì¶”ì²œìœ¼ë¡œ í´ë°±: {str(e)}")
        
        # ê¸°ë³¸ ì¶”ì²œ ëª¨ë“ˆ ì‚¬ìš©
        if JOB_RECOMMENDATION_AVAILABLE:
            recommendations = job_recommendation_module.get_recommendations_by_category(category, 10)
            
            if recommendations.get('status') == 'success':
                # ë°±ì—”ë“œ ì‘ë‹µ í˜•ì‹ì— ë§ì¶° ë³€í™˜
                posting_list = []
                
                base_ids = {
                    'ICT': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                    'BM': [1, 2, 3, 4, 5],
                    'SM': [1, 2, 3, 4, 5],
                    'PS': [1, 2, 3, 4, 5],
                    'RND': [1, 2, 3, 4, 5],
                    'ARD': [1, 2, 3, 4, 5],
                    'MM': [1, 2, 3, 4, 5]
                }
                
                category_ids = base_ids.get(category, base_ids['ICT'])
                
                for i, job in enumerate(recommendations.get('recommendations', [])):
                    job_posting_id = category_ids[i % len(category_ids)]
                    
                    posting_list.append({
                        "job_posting_id": job_posting_id,
                        "title": job.get('title', ''),
                        "keyword": ', '.join(job.get('keywords', [])),
                        "corporation": job.get('company', '')
                    })
                
                response = {"posting": posting_list}
                logger.info(f"ë°±ì—”ë“œ ê¸°ë³¸ ê³µê³ ì¶”ì²œ ì‘ë‹µ: {len(posting_list)}ê°œ ê³µê³ ")
                return jsonify(response)
        
        # ëª¨ë“  ì¶”ì²œì´ ì‹¤íŒ¨í•œ ê²½ìš°
        return jsonify({"error": "ì¶”ì²œ ìƒì„± ì‹¤íŒ¨"}), 500
            
    except Exception as e:
        error_msg = f"ë°±ì—”ë“œ ê³µê³ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

@app.route('/ai/jobs/recommend', methods=['POST'])
def recommend_jobs():
    """ê³µê³  ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸"""
    if not JOB_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        data = request.json or {}
        logger.info(f"ê³µê³  ì¶”ì²œ ìš”ì²­ ë°›ìŒ: {data}")
        
        if 'interview_scores' in data:
            interview_scores = data['interview_scores']
            limit = data.get('limit', 10)
            result = job_recommendation_module.get_recommendations_by_interview_result(interview_scores, limit)
            logger.info("ë©´ì ‘ ì ìˆ˜ ê¸°ë°˜ ì¶”ì²œ ì™„ë£Œ")
            return jsonify(result)
        elif 'category' in data:
            category = data['category']
            limit = data.get('limit', 10)
            result = job_recommendation_module.get_recommendations_by_category(category, limit)
            logger.info(f"{category} ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì™„ë£Œ")
            return jsonify(result)
        else:
            result = job_recommendation_module.get_recommendations_by_category("ICT", 10)
            logger.info("ê¸°ë³¸ ICT ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì™„ë£Œ")
            return jsonify(result)
            
    except Exception as e:
        error_msg = f"ê³µê³  ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

@app.route('/ai/jobs/categories', methods=['GET'])
def get_job_categories():
    """ê³µê³  ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    if not JOB_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        categories = {
            "BM": "ê²½ì˜/ê´€ë¦¬ì§",
            "SM": "ì˜ì—…/ë§ˆì¼€íŒ…",
            "PS": "ìƒì‚°/ê¸°ìˆ ì§", 
            "RND": "ì—°êµ¬ê°œë°œ",
            "ICT": "IT/ì •ë³´í†µì‹ ",
            "ARD": "ë””ìì¸/ì˜ˆìˆ ",
            "MM": "ë¯¸ë””ì–´/ë°©ì†¡"
        }
        
        return jsonify({
            "status": "success",
            "categories": categories,
            "total_count": len(categories)
        })
    except Exception as e:
        return jsonify({"error": f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/jobs/stats', methods=['GET'])  
def get_job_statistics():
    """ê³µê³  í†µê³„ ì •ë³´ ì¡°íšŒ"""
    if not JOB_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        result = job_recommendation_module.get_category_statistics()
        return jsonify(result)
    except Exception as e:
        error_msg = f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

# ==================== TF-IDF ê³µê³ ì¶”ì²œ API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.route('/ai/jobs/recommend-tfidf', methods=['POST'])
def recommend_jobs_tfidf():
    """TF-IDF ê¸°ë°˜ ê³µê³  ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸"""
    if not TFIDF_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        data = request.json or {}
        logger.info(f"TF-IDF ê³µê³  ì¶”ì²œ ìš”ì²­ ë°›ìŒ: {data}")
        
        # ê¸°ìˆ  ìŠ¤íƒ ê¸°ë°˜ ì¶”ì²œ
        if 'skills' in data:
            skills = data['skills']
            limit = data.get('limit', 10)
            result = tfidf_recommendation_module.get_recommendations_by_skills(skills, limit)
            logger.info(f"ê¸°ìˆ  ìŠ¤íƒ {skills} ê¸°ë°˜ TF-IDF ì¶”ì²œ ì™„ë£Œ")
            return jsonify(result)
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ ì¶”ì²œ
        elif 'profile' in data:
            profile = data['profile']
            limit = data.get('limit', 10)
            result = tfidf_recommendation_module.get_recommendations_by_profile(profile, limit)
            logger.info("í”„ë¡œí•„ ê¸°ë°˜ TF-IDF ì¶”ì²œ ì™„ë£Œ")
            return jsonify(result)
        
        else:
            return jsonify({"error": "skills ë˜ëŠ” profile ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
    except Exception as e:
        error_msg = f"TF-IDF ê³µê³  ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

@app.route('/ai/jobs/rare-skills', methods=['GET'])
def get_rare_skills():
    """í¬ì†Œ ê¸°ìˆ  ì •ë³´ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
    if not TFIDF_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        result = tfidf_recommendation_module.get_rare_skills_info()
        logger.info("í¬ì†Œ ê¸°ìˆ  ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
        return jsonify(result)
    except Exception as e:
        error_msg = f"í¬ì†Œ ê¸°ìˆ  ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

@app.route('/ai/recruitment/posting-tfidf', methods=['POST'])
def recruitment_posting_tfidf():
    """ë°±ì—”ë“œ ì—°ë™ìš© TF-IDF ê³µê³ ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸"""
    if not TFIDF_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        data = request.json or {}
        logger.info(f"ë°±ì—”ë“œ TF-IDF ê³µê³ ì¶”ì²œ ìš”ì²­ ë°›ìŒ: {data}")
        
        # RecruitmentRequest í˜•ì‹ìœ¼ë¡œ ìš”ì²­ ì²˜ë¦¬
        user_id = data.get('userId')
        profile = {
            'userId': user_id,
            'techStacks': data.get('techStacks', []),
            'certificateList': data.get('certificateList', []),
            'majorList': data.get('majorList', []),
            'careerYear': data.get('careerYear'),
            'educationLevel': data.get('educationLevel')
        }
        
        # TF-IDF ëª¨ë“ˆë¡œ ì¶”ì²œ ìƒì„±
        recommendations = tfidf_recommendation_module.get_recommendations_by_profile(profile, 10)
        
        if recommendations.get('status') == 'success':
            # ë°±ì—”ë“œ ì‘ë‹µ í˜•ì‹ì— ë§ì¶° ë³€í™˜ (ì´ë¯¸ ëª¨ë“ˆì—ì„œ ì²˜ë¦¬ë¨)
            response = {
                "posting": recommendations.get('posting', []),
                "totalCount": recommendations.get('totalCount', 0),
                "userId": user_id,
                "recommendationType": "tfidf",
                "generatedAt": recommendations.get('generatedAt')
            }
            
            logger.info(f"ë°±ì—”ë“œ TF-IDF ê³µê³ ì¶”ì²œ ì‘ë‹µ: {len(response.get('posting', []))}ê°œ ê³µê³ ")
            return jsonify(response)
        else:
            return jsonify({"error": "ì¶”ì²œ ìƒì„± ì‹¤íŒ¨"}), 500
            
    except Exception as e:
        error_msg = f"ë°±ì—”ë“œ TF-IDF ê³µê³ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

@app.route('/ai/jobs/crawl-trigger', methods=['POST'])
def trigger_job_crawling():
    """ì±„ìš©ê³µê³  í¬ë¡¤ë§ íŠ¸ë¦¬ê±° ì—”ë“œí¬ì¸íŠ¸"""
    if not TFIDF_RECOMMENDATION_AVAILABLE:
        return jsonify({"error": "TF-IDF ê³µê³ ì¶”ì²œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
    
    try:
        result = tfidf_recommendation_module.trigger_crawling()
        logger.info("ì±„ìš©ê³µê³  í¬ë¡¤ë§ íŠ¸ë¦¬ê±° ì™„ë£Œ")
        return jsonify(result)
    except Exception as e:
        error_msg = f"í¬ë¡¤ë§ íŠ¸ë¦¬ê±° ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        return jsonify({"error": error_msg}), 500

# ==================== ê°œì¸ë©´ì ‘ API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.route('/ai/interview/start', methods=['POST'])
def start_interview():
    """ê°œì¸ë©´ì ‘ ì‹œì‘ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        data = request.json or {}
        logger.info(f"ê°œì¸ë©´ì ‘ ì‹œì‘ ìš”ì²­: {data}")
        
        return jsonify({
            "status": "success",
            "interview_id": int(time.time()),
            "message": "ê°œì¸ë©´ì ‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "first_question": "ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
            "server_type": "main_server"
        })
    except Exception as e:
        return jsonify({"error": f"ê°œì¸ë©´ì ‘ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/interview/<int:interview_id>/question', methods=['GET'])
def get_interview_question(interview_id):
    """ë©´ì ‘ ì§ˆë¬¸ ìƒì„±"""
    try:
        question_type = request.args.get('type', 'general')
        
        # LLMì„ ì‚¬ìš©í•œ ì§ˆë¬¸ ìƒì„± (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if LLM_MODULE_AVAILABLE and llm_module:
            # ì‹¤ì œ LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ë¡œì§
            questions = {
                "general": "ìì‹ ì˜ ì¥ì ê³¼ ë‹¨ì ì„ êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "technical": "ìµœê·¼ ê´€ì‹¬ìˆê²Œ ì§€ì¼œë³´ê³  ìˆëŠ” ê¸°ìˆ  íŠ¸ë Œë“œê°€ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ì§€, ê·¸ë¦¬ê³  ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "behavioral": "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ì˜ê²¬ ì¶©ëŒì´ ë°œìƒí–ˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ì§€ êµ¬ì²´ì ì¸ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”."
            }
        else:
            # ê¸°ë³¸ ì§ˆë¬¸
            questions = {
                "general": "ë³¸ì¸ì˜ ì¥ì ê³¼ ë‹¨ì ì— ëŒ€í•´ ë§ì”€í•´ì£¼ì„¸ìš”.",
                "technical": "ìµœê·¼ì— ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆëŠ” ê¸°ìˆ  íŠ¸ë Œë“œê°€ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”?",
                "behavioral": "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ê°ˆë“±ì´ ìƒê²¼ì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ì…¨ë‚˜ìš”?"
            }
        
        return jsonify({
            "interview_id": interview_id,
            "question": questions.get(question_type, questions["general"]),
            "question_type": question_type,
            "generated_by": "llm" if LLM_MODULE_AVAILABLE else "template"
        })
    except Exception as e:
        return jsonify({"error": f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/interview/<int:interview_id>/answer-video', methods=['POST'])
def process_interview_answer(interview_id):
    """ê°œì¸ë©´ì ‘ ë‹µë³€ ì˜ìƒ ì²˜ë¦¬"""
    try:
        if 'file' not in request.files and 'video' not in request.files:
            return jsonify({"error": "ì˜ìƒ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        file = request.files.get('file') or request.files.get('video')
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_path = f"temp_interview_{interview_id}_{int(time.time())}.mp4"
        file.save(temp_path)
        
        try:
            # ì˜¤ë””ì˜¤ ì¶”ì¶œ
            audio_path = extract_audio_from_video(temp_path)
            
            # Whisper ìŒì„± ì¸ì‹
            transcription_result = {"text": "ë‹µë³€ ë‚´ìš©ì´ ì¸ì‹ë˜ì—ˆìŠµë‹ˆë‹¤.", "confidence": 0.85}
            if audio_path and WHISPER_AVAILABLE:
                transcription_result = transcribe_with_whisper(audio_path)
            
            # Librosa ì˜¤ë””ì˜¤ ë¶„ì„
            audio_analysis = {"voice_stability": 0.8, "fluency_score": 0.85}
            if audio_path and LIBROSA_AVAILABLE:
                audio_analysis = process_audio_with_librosa(audio_path)
            
            # OpenFace ì–¼êµ´ ë¶„ì„ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            facial_analysis = {"confidence": 0.8, "emotion": "ì¤‘ë¦½"}
            if OPENFACE_INTEGRATION_AVAILABLE and openface_integration:
                try:
                    facial_analysis = openface_integration.analyze_video(temp_path)
                except Exception as e:
                    logger.warning(f"OpenFace ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            
            # ì¢…í•© ë¶„ì„ ê²°ê³¼
            result = {
                "interview_id": interview_id,
                "transcription": transcription_result.get("text", ""),
                "transcription_confidence": transcription_result.get("confidence", 0.85),
                "analysis": {
                    "content_score": calculate_content_score(transcription_result.get("text", "")),
                    "voice_score": audio_analysis.get("voice_stability", 0.8) * 5,
                    "action_score": facial_analysis.get("confidence", 0.8) * 5,
                    "fluency": audio_analysis.get("fluency_score", 0.85),
                    "emotion": facial_analysis.get("emotion", "ì¤‘ë¦½")
                },
                "feedback": generate_interview_feedback(transcription_result, audio_analysis, facial_analysis),
                "next_question": "ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ì‹œê² ìŠµë‹ˆê¹Œ?",
                "processing_methods": {
                    "transcription": "whisper" if WHISPER_AVAILABLE else "fallback",
                    "audio_analysis": "librosa" if LIBROSA_AVAILABLE else "fallback",
                    "facial_analysis": "openface" if OPENFACE_INTEGRATION_AVAILABLE else "fallback"
                }
            }
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            cleanup_temp_files([temp_path, audio_path])
            
            return jsonify(result)
            
        except Exception as e:
            cleanup_temp_files([temp_path])
            raise e
        
    except Exception as e:
        return jsonify({"error": f"ë‹µë³€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

def calculate_content_score(text: str) -> float:
    """ë‚´ìš© ì ìˆ˜ ê³„ì‚°"""
    if not text:
        return 2.0
    
    # ê¸°ë³¸ ì ìˆ˜
    base_score = 3.0
    
    # ê¸¸ì´ ë³´ë„ˆìŠ¤
    word_count = len(text.split())
    if word_count > 50:
        base_score += 1.0
    elif word_count > 30:
        base_score += 0.5
    
    # êµ¬ì²´ì„± ë³´ë„ˆìŠ¤
    concrete_words = ["ì˜ˆë¥¼ ë“¤ì–´", "êµ¬ì²´ì ìœ¼ë¡œ", "ì‹¤ì œë¡œ", "ê²½í—˜", "ì‚¬ë¡€"]
    if any(word in text for word in concrete_words):
        base_score += 0.5
    
    return min(5.0, base_score)

def generate_interview_feedback(transcription: Dict, audio: Dict, facial: Dict) -> str:
    """ë©´ì ‘ í”¼ë“œë°± ìƒì„±"""
    try:
        text = transcription.get("text", "")
        confidence = transcription.get("confidence", 0.85)
        voice_stability = audio.get("voice_stability", 0.8)
        
        feedback_parts = []
        
        # ë‚´ìš© í”¼ë“œë°±
        if len(text) > 100:
            feedback_parts.append("ì¶©ë¶„í•œ ë‚´ìš©ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì…¨ìŠµë‹ˆë‹¤.")
        else:
            feedback_parts.append("ì¢€ ë” êµ¬ì²´ì ì¸ ë‹µë³€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ìŒì„± í”¼ë“œë°±
        if voice_stability > 0.8:
            feedback_parts.append("ì•ˆì •ì ì¸ ìŒì„±ìœ¼ë¡œ ë°œí‘œí•˜ì…¨ìŠµë‹ˆë‹¤.")
        else:
            feedback_parts.append("ìŒì„±ì˜ ì•ˆì •ì„±ì„ ë†’ì—¬ë³´ì„¸ìš”.")
        
        # ì‹ ë¢°ë„ í”¼ë“œë°±
        if confidence > 0.8:
            feedback_parts.append("ëª…í™•í•œ ë°œìŒìœ¼ë¡œ ì „ë‹¬í•˜ì…¨ìŠµë‹ˆë‹¤.")
        
        return " ".join(feedback_parts)
        
    except Exception as e:
        logger.error(f"í”¼ë“œë°± ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return "ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ë‹µë³€ì´ì—ˆìŠµë‹ˆë‹¤."

def cleanup_temp_files(file_paths: list):
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    for file_path in file_paths:
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception as e:
                logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {str(e)}")

# ==================== í† ë¡ ë©´ì ‘ API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.route('/ai/debate/<int:debate_id>/ai-opening', methods=['POST'])
def ai_opening(debate_id):
    """AI ì…ë¡  ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    try:
        data = request.json or {}
        logger.info(f"AI ì…ë¡  ìƒì„± ìš”ì²­: /ai/debate/{debate_id}/ai-opening")
        
        topic = data.get("topic", "ì¸ê³µì§€ëŠ¥")
        position = data.get("position", "CON")
        
        # LLMì„ ì‚¬ìš©í•œ ì…ë¡  ìƒì„± (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if LLM_MODULE_AVAILABLE and llm_module:
            try:
                llm_result = llm_module.generate_ai_opening(topic, position, data)
                ai_response = llm_result.get("ai_response", "")
                
                # TTS ë³€í™˜ (ì„ íƒì )
                if TTS_AVAILABLE and len(ai_response) > 0:
                    tts_path = f"temp_tts_{debate_id}_{int(time.time())}.wav"
                    synthesize_with_tts(ai_response, tts_path)
                
                return jsonify({
                    "ai_opening_text": ai_response,
                    "generated_by": "llm",
                    "reasoning": llm_result.get("reasoning", ""),
                    "timestamp": llm_result.get("timestamp")
                })
                
            except Exception as e:
                logger.error(f"LLM ì…ë¡  ìƒì„± ì˜¤ë¥˜: {str(e)}")
                # ë°±ì—… ì‘ë‹µìœ¼ë¡œ í´ë°±
        
        # ë°±ì—… ì‘ë‹µ (LLM ì‚¬ìš© ë¶ˆê°€ì‹œ)
        fallback_responses = {
            "PRO": f"{topic}ì˜ ë°œì „ì€ ì¸ë¥˜ì—ê²Œ ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤. íš¨ìœ¨ì„± ì¦ëŒ€ì™€ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.",
            "CON": f"{topic}ì— ëŒ€í•´ì„œëŠ” ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ë¶€ì‘ìš©ê³¼ ìœ„í—˜ì„±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤."
        }
        
        ai_response = fallback_responses.get(position, fallback_responses["CON"])
        
        return jsonify({
            "ai_opening_text": ai_response,
            "generated_by": "fallback",
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        })
        
    except Exception as e:
        return jsonify({"error": f"AI ì…ë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/debate/<int:debate_id>/opening-video', methods=['POST'])
def process_opening_video(debate_id):
    """ì‚¬ìš©ì ì…ë¡  ì˜ìƒ ì²˜ë¦¬"""
    return process_debate_video_generic(debate_id, "opening", "rebuttal")

@app.route('/ai/debate/<int:debate_id>/rebuttal-video', methods=['POST'])
def process_rebuttal_video(debate_id):
    """ì‚¬ìš©ì ë°˜ë¡  ì˜ìƒ ì²˜ë¦¬"""
    return process_debate_video_generic(debate_id, "rebuttal", "counter_rebuttal")

@app.route('/ai/debate/<int:debate_id>/counter-rebuttal-video', methods=['POST'])
def process_counter_rebuttal_video(debate_id):
    """ì‚¬ìš©ì ì¬ë°˜ë¡  ì˜ìƒ ì²˜ë¦¬"""
    return process_debate_video_generic(debate_id, "counter_rebuttal", "closing")

@app.route('/ai/debate/<int:debate_id>/closing-video', methods=['POST'])
def process_closing_video(debate_id):
    """ì‚¬ìš©ì ìµœì¢… ë³€ë¡  ì˜ìƒ ì²˜ë¦¬"""
    return process_debate_video_generic(debate_id, "closing", None)

def process_debate_video_generic(debate_id, current_stage, next_ai_stage):
    """í† ë¡  ì˜ìƒ ì²˜ë¦¬ ê³µí†µ í•¨ìˆ˜"""
    logger.info(f"{current_stage} ì˜ìƒ ì²˜ë¦¬ ìš”ì²­: /ai/debate/{debate_id}/{current_stage}-video")
    
    if 'file' not in request.files and 'video' not in request.files:
        return jsonify({"error": "ì˜ìƒ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
    
    file = request.files.get('file') or request.files.get('video')
    
    try:
        temp_path = f"temp_{current_stage}_{debate_id}_{int(time.time())}.mp4"
        file.save(temp_path)
        
        # ì˜¤ë””ì˜¤ ì¶”ì¶œ
        audio_path = extract_audio_from_video(temp_path)
        
        # Whisper ìŒì„± ì¸ì‹
        transcription_result = {"text": f"ì‚¬ìš©ìì˜ {current_stage} ë°œì–¸ì…ë‹ˆë‹¤.", "confidence": 0.85}
        if audio_path and WHISPER_AVAILABLE:
            transcription_result = transcribe_with_whisper(audio_path)
        
        # Librosa ì˜¤ë””ì˜¤ ë¶„ì„
        audio_analysis = {"voice_stability": 0.8, "fluency_score": 0.85}
        if audio_path and LIBROSA_AVAILABLE:
            audio_analysis = process_audio_with_librosa(audio_path)
        
        # OpenFace ì–¼êµ´ ë¶„ì„
        facial_analysis = {"confidence": 0.8, "emotion": "ì¤‘ë¦½"}
        if OPENFACE_INTEGRATION_AVAILABLE and openface_integration:
            try:
                facial_analysis = openface_integration.analyze_video(temp_path)
            except Exception as e:
                logger.warning(f"OpenFace ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        scores = calculate_debate_scores(transcription_result, audio_analysis, facial_analysis)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            f"user_{current_stage}_text": transcription_result.get("text", ""),
            "emotion": facial_analysis.get("emotion", "ì¤‘ë¦½ (ì•ˆì •ì )"),
            **scores
        }
        
        # ë‹¤ìŒ AI ì‘ë‹µ ìƒì„±
        if next_ai_stage:
            topic = request.form.get("topic", "ì¸ê³µì§€ëŠ¥")
            user_text = transcription_result.get("text", "")
            
            if LLM_MODULE_AVAILABLE and llm_module:
                try:
                    # LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
                    llm_result = llm_module.analyze_user_response_and_generate_rebuttal(
                        user_text, facial_analysis, audio_analysis, next_ai_stage
                    )
                    ai_response = llm_result.get("ai_response", "")
                    result[f"ai_{next_ai_stage}_text"] = ai_response
                    result["ai_analysis"] = llm_result.get("analysis_insights", {})
                    
                except Exception as e:
                    logger.error(f"LLM ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    # ë°±ì—… ì‘ë‹µ ì‚¬ìš©
                    result[f"ai_{next_ai_stage}_text"] = get_fallback_ai_response(next_ai_stage, topic, user_text)
            else:
                # ë°±ì—… ì‘ë‹µ
                result[f"ai_{next_ai_stage}_text"] = get_fallback_ai_response(next_ai_stage, topic, user_text)
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        cleanup_temp_files([temp_path, audio_path])
        
        return jsonify(result)
        
    except Exception as e:
        cleanup_temp_files([temp_path])
        return jsonify({"error": f"ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

def calculate_debate_scores(transcription: Dict, audio: Dict, facial: Dict) -> Dict[str, Any]:
    """í† ë¡  ì ìˆ˜ ê³„ì‚°"""
    try:
        text = transcription.get("text", "")
        confidence = transcription.get("confidence", 0.85)
        voice_stability = audio.get("voice_stability", 0.8)
        fluency = audio.get("fluency_score", 0.85)
        facial_confidence = facial.get("confidence", 0.8)
        
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        scores = {
            "initiative_score": min(5.0, 3.0 + confidence),
            "collaborative_score": min(5.0, 3.0 + (len(text.split()) / 100)),
            "communication_score": min(5.0, 3.0 + fluency),
            "logic_score": min(5.0, 3.0 + calculate_logic_score(text)),
            "problem_solving_score": min(5.0, 3.0 + calculate_problem_solving_score(text)),
            "voice_score": min(5.0, voice_stability * 5),
            "action_score": min(5.0, facial_confidence * 5)
        }
        
        # í”¼ë“œë°± ìƒì„±
        feedback_mapping = {
            "initiative": "ì ê·¹ì„±",
            "collaborative": "í˜‘ë ¥ì„±", 
            "communication": "ì˜ì‚¬ì†Œí†µ",
            "logic": "ë…¼ë¦¬ì„±",
            "problem_solving": "ë¬¸ì œí•´ê²°",
            "voice": "ìŒì„±í’ˆì§ˆ",
            "action": "í–‰ë™í‘œí˜„"
        }
        
        for category, korean_name in feedback_mapping.items():
            score = scores[f"{category}_score"]
            if score >= 4.5:
                scores[f"{category}_feedback"] = f"{korean_name}: ë§¤ìš° ìš°ìˆ˜"
            elif score >= 4.0:
                scores[f"{category}_feedback"] = f"{korean_name}: ìš°ìˆ˜"
            elif score >= 3.5:
                scores[f"{category}_feedback"] = f"{korean_name}: ì–‘í˜¸"
            else:
                scores[f"{category}_feedback"] = f"{korean_name}: ê°œì„  í•„ìš”"
        
        # ì¢…í•© í”¼ë“œë°±
        avg_score = sum(scores[k] for k in scores if k.endswith("_score")) / 7
        if avg_score >= 4.5:
            scores["feedback"] = "ë§¤ìš° ìš°ìˆ˜í•œ í† ë¡  ìˆ˜í–‰ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
        elif avg_score >= 4.0:
            scores["feedback"] = "ìš°ìˆ˜í•œ í† ë¡  ìˆ˜í–‰ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
        elif avg_score >= 3.5:
            scores["feedback"] = "ì–‘í˜¸í•œ í† ë¡  ìˆ˜í–‰ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
        else:
            scores["feedback"] = "í† ë¡  ìˆ˜í–‰ì— ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        scores["sample_answer"] = "êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì˜ˆì‹œë¥¼ ë“¤ì–´ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
        
        return scores
        
    except Exception as e:
        logger.error(f"í† ë¡  ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        return get_default_debate_scores()

def calculate_logic_score(text: str) -> float:
    """ë…¼ë¦¬ì„± ì ìˆ˜ ê³„ì‚°"""
    logic_indicators = ["ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê²°ë¡ ì ìœ¼ë¡œ", "ì™œëƒí•˜ë©´", "ê·¼ê±°ëŠ”", "ì˜ˆë¥¼ ë“¤ì–´"]
    logic_count = sum(1 for indicator in logic_indicators if indicator in text)
    return min(1.0, logic_count * 0.3)

def calculate_problem_solving_score(text: str) -> float:
    """ë¬¸ì œí•´ê²° ì ìˆ˜ ê³„ì‚°"""
    problem_solving_indicators = ["í•´ê²°", "ë°©ì•ˆ", "ëŒ€ì•ˆ", "ê°œì„ ", "ì „ëµ", "ì ‘ê·¼"]
    problem_count = sum(1 for indicator in problem_solving_indicators if indicator in text)
    return min(1.0, problem_count * 0.3)

def get_default_debate_scores() -> Dict[str, Any]:
    """ê¸°ë³¸ í† ë¡  ì ìˆ˜"""
    return {
        "initiative_score": 3.5,
        "collaborative_score": 3.2,
        "communication_score": 3.8,
        "logic_score": 3.3,
        "problem_solving_score": 3.5,
        "voice_score": 3.7,
        "action_score": 3.9,
        "initiative_feedback": "ì ê·¹ì„±: ë³´í†µ",
        "collaborative_feedback": "í˜‘ë ¥ì„±: ë³´í†µ",
        "communication_feedback": "ì˜ì‚¬ì†Œí†µ: ì–‘í˜¸",
        "logic_feedback": "ë…¼ë¦¬ì„±: ë³´í†µ",
        "problem_solving_feedback": "ë¬¸ì œí•´ê²°: ë³´í†µ",
        "voice_feedback": "ìŒì„±í’ˆì§ˆ: ì–‘í˜¸",
        "action_feedback": "í–‰ë™í‘œí˜„: ì–‘í˜¸",
        "feedback": "ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ìˆ˜í–‰ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.",
        "sample_answer": "êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ê·¼ê±°ë¥¼ ë“¤ì–´ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•´ë³´ì„¸ìš”."
    }

def get_fallback_ai_response(stage: str, topic: str = "ì¸ê³µì§€ëŠ¥", user_text: str = "") -> str:
    """ë°±ì—… AI ì‘ë‹µ"""
    responses = {
        "rebuttal": f"ë§ì”€í•˜ì‹  {topic}ì— ëŒ€í•œ ì˜ê²¬ì„ ê²½ì²­í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë³´ë©´ ëª‡ ê°€ì§€ ê³ ë ¤í•´ì•¼ í•  ì ë“¤ì´ ìˆìŠµë‹ˆë‹¤.",
        "counter_rebuttal": f"ì œê¸°í•˜ì‹  ì ë“¤ì„ ì¶©ë¶„íˆ ì´í•´í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ {topic}ì˜ ì¥ê¸°ì  ê´€ì ì—ì„œ ë³´ë©´ ì—¬ì „íˆ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "closing": f"ê²°ë¡ ì ìœ¼ë¡œ {topic}ì— ëŒ€í•œ ì´ë²ˆ í† ë¡ ì„ í†µí•´ ë‹¤ì–‘í•œ ê´€ì ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê· í˜•ì¡íŒ ì‹œê°ì´ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤."
    }
    
    return responses.get(stage, f"{topic}ì— ëŒ€í•´ ì˜ê²¬ì„ ë‚˜ëˆ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.")

# AI ë‹¨ê³„ë³„ ì‘ë‹µ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.route('/ai/debate/<int:debate_id>/ai-rebuttal', methods=['GET'])
def ai_rebuttal(debate_id):
    """AI ë°˜ë¡  ìƒì„±"""
    try:
        topic = request.args.get("topic", "ì¸ê³µì§€ëŠ¥")
        ai_response = get_fallback_ai_response("rebuttal", topic)
        return jsonify({"debate_id": debate_id, "ai_rebuttal_text": ai_response})
    except Exception as e:
        return jsonify({"error": f"AI ë°˜ë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/debate/<int:debate_id>/ai-counter-rebuttal', methods=['GET'])
def ai_counter_rebuttal(debate_id):
    """AI ì¬ë°˜ë¡  ìƒì„±"""
    try:
        topic = request.args.get("topic", "ì¸ê³µì§€ëŠ¥")
        ai_response = get_fallback_ai_response("counter_rebuttal", topic)
        return jsonify({"debate_id": debate_id, "ai_counter_rebuttal_text": ai_response})
    except Exception as e:
        return jsonify({"error": f"AI ì¬ë°˜ë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/debate/<int:debate_id>/ai-closing', methods=['GET'])
def ai_closing(debate_id):
    """AI ìµœì¢… ë³€ë¡  ìƒì„±"""
    try:
        topic = request.args.get("topic", "ì¸ê³µì§€ëŠ¥")
        ai_response = get_fallback_ai_response("closing", topic)
        return jsonify({"debate_id": debate_id, "ai_closing_text": ai_response})
    except Exception as e:
        return jsonify({"error": f"AI ìµœì¢… ë³€ë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}), 500

@app.route('/ai/debate/modules-status', methods=['GET'])
def modules_status():
    """ëª¨ë“ˆ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "active",
        "server_type": "main_server",
        "modules": {
            "llm": {
                "available": LLM_MODULE_AVAILABLE,
                "provider": "openai" if LLM_MODULE_AVAILABLE else None
            },
            "openface": {
                "available": OPENFACE_INTEGRATION_AVAILABLE,
                "functions": ["analyze_video", "extract_features"] if OPENFACE_INTEGRATION_AVAILABLE else []
            },
            "whisper": {
                "available": WHISPER_AVAILABLE,
                "functions": ["transcribe_video"] if WHISPER_AVAILABLE else []
            },
            "tts": {
                "available": TTS_AVAILABLE,
                "functions": ["text_to_speech"] if TTS_AVAILABLE else []
            },
            "librosa": {
                "available": LIBROSA_AVAILABLE,
                "functions": ["analyze_audio", "extract_features"] if LIBROSA_AVAILABLE else []
            },
            "job_recommendation": {
                "available": JOB_RECOMMENDATION_AVAILABLE,
                "functions": ["recommend_jobs", "get_categories"] if JOB_RECOMMENDATION_AVAILABLE else []
            },
            "tfidf_recommendation": {
                "available": TFIDF_RECOMMENDATION_AVAILABLE,
                "functions": ["recommend_by_skills", "recommend_by_profile", "get_rare_skills", "trigger_crawling"] if TFIDF_RECOMMENDATION_AVAILABLE else []
            }
        }
    })

if __name__ == "__main__":
    # ì‹œì‘ ì‹œ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    initialize_ai_systems()
    
    print("ğŸš€ VeriView AI ë©”ì¸ ì„œë²„ ì‹œì‘...")
    print("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://localhost:5000")
    print("ğŸ” í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸: http://localhost:5000/ai/test")
    print("ğŸ“Š ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸: http://localhost:5000/ai/debate/modules-status")
    print("ğŸ¯ í† ë¡ ë©´ì ‘ API: http://localhost:5000/ai/debate/<debate_id>/ai-opening")
    print("ğŸ’¼ ê°œì¸ë©´ì ‘ API: http://localhost:5000/ai/interview/start")
    print("ğŸ“‹ ê³µê³ ì¶”ì²œ API: http://localhost:5000/ai/jobs/recommend")
    print("ğŸ¯ TF-IDF ê³µê³ ì¶”ì²œ API: http://localhost:5000/ai/jobs/recommend-tfidf")
    print("ğŸ” í¬ì†Œê¸°ìˆ  ì •ë³´ API: http://localhost:5000/ai/jobs/rare-skills")
    print("=" * 80)
    print("í¬í•¨ëœ AI ëª¨ë“ˆ:")
    print(f"  - LLM: {'âœ…' if LLM_MODULE_AVAILABLE else 'âŒ'}")
    print(f"  - OpenFace: {'âœ…' if OPENFACE_INTEGRATION_AVAILABLE else 'âŒ'}")
    print(f"  - Whisper: {'âœ…' if WHISPER_AVAILABLE else 'âŒ'}")
    print(f"  - TTS: {'âœ…' if TTS_AVAILABLE else 'âŒ'}")
    print(f"  - Librosa: {'âœ…' if LIBROSA_AVAILABLE else 'âŒ'}")
    print(f"  - ê³µê³ ì¶”ì²œ: {'âœ…' if JOB_RECOMMENDATION_AVAILABLE else 'âŒ'}")
    print(f"  - TF-IDF ê³µê³ ì¶”ì²œ: {'âœ…' if TFIDF_RECOMMENDATION_AVAILABLE else 'âŒ'}")
    print("=" * 80)
    if TFIDF_RECOMMENDATION_AVAILABLE:
        print("íŠ¹ë³„ ê¸°ëŠ¥:")
        print("  - í¬ì†Œ ê¸°ìˆ  ë³´ë„ˆìŠ¤ ì‹œìŠ¤í…œ (PyTorch, NVIDIA DeepStream ë“±)")
        print("  - TF-IDF ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë°€ ë§¤ì¹­")
        print("  - ë°±ì—”ë“œ ì™„ì „ í˜¸í™˜ (JobPosting ì—”í‹°í‹°)")
        print("="*80)
    
    app.run(host="0.0.0.0", port=5000, debug=True)
